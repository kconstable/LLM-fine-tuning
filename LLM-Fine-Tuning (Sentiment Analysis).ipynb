{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"machine_shape":"hm","gpuType":"T4","mount_file_id":"1iEJHPslYHH9FD7tXI_FBAWGHDEUHweZ_","authorship_tag":"ABX9TyMH95juuaz4UMnPA0jzEp5K"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Text Classification Fine Tuning: DistilBERT"],"metadata":{"id":"-knib2KqzcEa"}},{"cell_type":"markdown","source":["## Resources\n","- Verify the availability of notebook resources\n","- Fine-tuning necessitates the use of either a GPU or a TPU"],"metadata":{"id":"3OwTg8VgjoIx"}},{"cell_type":"code","source":["# display resources\n","gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i0O-zDKFn0ya","executionInfo":{"status":"ok","timestamp":1718229202782,"user_tz":240,"elapsed":462,"user":{"displayName":"Ken Constable","userId":"07550562182491091576"}},"outputId":"87af2082-3c2d-4d03-9bf4-9cead2e1c716"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Wed Jun 12 21:53:22 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   49C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|  No running processes found                                                           |\n","+---------------------------------------------------------------------------------------+\n"]}]},{"cell_type":"markdown","source":["### Install Libraries\n","+ Hugging Face\n","+ PyTorch\n","+ Standard Python data science libraries"],"metadata":{"id":"BNc2M2PiT4_T"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"FadF0CA7ivau"},"outputs":[],"source":["%pip install transformers datasets evaluate accelerate pipeline bitsandbytes\n","%pip install torch torchdata\n","%pip install peft\n","%pip install loralib\n","%pip install huggingface_hub"]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import random\n","from datasets import load_dataset\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForSeq2SeqLM,\n","    AutoModelForCausalLM,\n","    AutoModelForSequenceClassification,\n","    GenerationConfig,\n","    TrainingArguments,\n","    Trainer,\n","    pipeline,\n","    BitsAndBytesConfig,\n","    DataCollatorForSeq2Seq,\n","    DataCollatorWithPadding\n",")\n","import torch\n","import evaluate\n","from peft import (\n","    LoraConfig,\n","    get_peft_model,\n","    TaskType,\n","    PeftModel,\n","    PeftConfig,\n",")\n","from huggingface_hub import login"],"metadata":{"id":"edwnrK-Vj0kv","executionInfo":{"status":"ok","timestamp":1718229601341,"user_tz":240,"elapsed":430,"user":{"displayName":"Ken Constable","userId":"07550562182491091576"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["### HuggingFace Authentication\n","+ Authenticate to pull models and datasets (read token required)\n","+ Authenticate to push models to hugging face hub (write token required)"],"metadata":{"id":"f_ifHfARmcYg"}},{"cell_type":"code","source":["login()"],"metadata":{"id":"UNYkdQnumI57"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Notebook Config\n","+ Define some useful constants\n","+ Device (CPU or CUDA for distributed environments)\n","+ Model Paths (saving model checkpoints, adaptor weights)"],"metadata":{"id":"vYI26pCHUMNM"}},{"cell_type":"code","source":["# training directory\n","MNAME = 'sentiment'\n","DIR_MODEL = f\"/content/drive/MyDrive/Colab Notebooks/fine-tuning-llm/{MNAME}/peft/models/\"\n","\n","# device\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","DEVICE"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"00VSWiobj3BM","executionInfo":{"status":"ok","timestamp":1718229434079,"user_tz":240,"elapsed":260,"user":{"displayName":"Ken Constable","userId":"07550562182491091576"}},"outputId":"64c8a36d-f124-4105-ac5b-8d3d6a81a9a9"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["## DistilBERT\n","+ Distilled version of BERT (by Google)\n","+ [DistilBERT](https://huggingface.co/docs/transformers/en/model_doc/distilbert)\n","+ [distilBERT-base-uncased](https://huggingface.co/distilbert/distilbert-base-uncased) - The base model selected for fine-tuning (case insensitive)\n","\n","#### Why DistilBERT?\n","+ Smaller and faster than BERT (40% fewer parameters)\n","+ Runs 60% faster than BERT\n","+ Preserves 95% of BERT's performance\n","+ Well suited for text classification"],"metadata":{"id":"M2HyxCDD7hew"}},{"cell_type":"markdown","source":["### Fine Tuning Dataset: IMDB\n","+ [imdb](https://huggingface.co/datasets/stanfordnlp/imdb) => available from HuggingFace\n","+ Movie review data set(25k train, 25k test)\n","+ Consists of a review (text) and a human-assigned label (1=positive, 0=negative)\n","+ Steps:\n","  + The dataset is split between Train | Test\n","  + The Test dataset was further split into Train | Validate for training\n","  + The number of observations was randomly subset to reduce the compute time required for fine-tuning (Train 1k, Test 1k, Validate 0.5k)"],"metadata":{"id":"Y6xFI3gdLKqr"}},{"cell_type":"code","source":["# classification dataset\n","data_imdb = load_dataset(\"imdb\")\n","\n","# split in to train, test, validate\n","data_train = data_imdb['train']\n","\n","# split test into test, validate\n","data_test = data_imdb['test'].train_test_split(test_size=0.3)\n","\n","# subset rows to reduce train time\n","train = data_train.shuffle(seed=1985).select([idx for idx in list(range(1000))])\n","test = data_test['train'].shuffle(seed=1985).select([idx for idx in list(range(1000))])\n","validate = data_test['test'].shuffle(seed=1985).select([idx for idx in list(range(100))])\n"],"metadata":{"id":"ONaTTiaOxg8n","executionInfo":{"status":"ok","timestamp":1718236812160,"user_tz":240,"elapsed":2370,"user":{"displayName":"Ken Constable","userId":"07550562182491091576"}}},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":["### Base Model\n","+ The distilBERT base model (case insensitive version) was fine-tuned with the imdb data to improve text classification (positive, negative)\n","+ Source: HuggingFace\n","+ Implementation: HuggingFace, Torch\n","+ Steps:\n"," + Download the pre-trained model\n"," + Create a tokenizer\n"," + Encode/decode the labels (1:positive, 0:negative)\n"," + Define the number number of labels (binary classification)\n"," + Define the base model (for fine-tuning)\n"," + Define the original model (for evaluation)\n"," + Move the models to the DEVICE (cpu, cuda)\n","\n"],"metadata":{"id":"C-O9fuwrGa5U"}},{"cell_type":"code","source":["# DistilBERT Base Model\n","base_model_name = 'distilbert-base-uncased'\n","tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n","\n","# classification mappings\n","id2label = {0:\"Negative\",1:\"Positive\"}\n","label2id = {\"Negative\":0, \"Positive\":1}\n","\n","# base model for training\n","base_model = AutoModelForSequenceClassification.from_pretrained(\n","    base_model_name,\n","    num_labels=2,\n","    id2label=id2label,\n","    label2id=label2id,\n","    torch_dtype=torch.bfloat16\n","    ).to(DEVICE)\n","\n","# original model for evaluation\n","original_model = AutoModelForSequenceClassification.from_pretrained(\n","    base_model_name,\n","    num_labels=2,\n","    id2label=id2label,\n","    label2id=label2id,\n","    torch_dtype=torch.bfloat16\n","    ).to(DEVICE)\n"],"metadata":{"id":"n4vDRZv27t0v"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Preprocessing\n","+ Preprocessing is required to tokenize the inputs and standardize the length of each review.\n","+ Steps:\n","  + Tokenize each review\n","  + Standardize review length: A combination of truncation and padding was used to ensure the length of text for each review was the same length.\n","  + The DataCollatorWithPadding function from HuggingFace was used to automatically set padding levels during training."],"metadata":{"id":"nCa7nybxGgAe"}},{"cell_type":"code","source":["def preprocess(examples):\n","  \"\"\" Tokenize the input text \"\"\"\n","  tokens = tokenizer(examples['text'], truncation=True)\n","  return tokens\n","\n","# preprocess each review in the train, test and validate datasets\n","tokenized_train = train.map(preprocess, batched=True)\n","tokenized_test = test.map(preprocess, batched=True)\n","tokenized_val = validate.map(preprocess, batched=True)"],"metadata":{"id":"9i6E4jcOe2dD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Generate Responses\n","+ Deinfe a convenience function to generate a classification for a sample review from the dataset\n","+ Steps\n","\t+ Tokenize the review\n","\t+ Generate a response\n","\t+ Extract the logits\n","\t+ Infer the classification from the maximum logit value\n","\t+ Optionally print the review, the decoded classification, and human labels"],"metadata":{"id":"Q2JBgKBlk9p-"}},{"cell_type":"code","source":["def get_response(example, model, tokenizer, verbose=False):\n","  \"\"\" Generate a classification for a sample review \"\"\"\n","  # tokenize the input text\n","  encoded_input = tokenizer(example['text'], return_tensors=\"pt\", truncation=True, padding =True)\n","  encoded_input.to(DEVICE)\n","\n","  # get the logits\n","  logits = model(**encoded_input).logits\n","\n","  # classify\n","  prediction = torch.argmax(logits).tolist()\n","\n","  # print a summary\n","  if verbose:\n","    # decode the prediction\n","    decoded_output = id2label[prediction]\n","    print(\"Input Text\")\n","    print(\"=\"*100)\n","    print(example['text'])\n","    print(\"=\"*100)\n","    print(f\"Prediction: {decoded_output} | Label: {id2label[example['label']]}\")\n","  else:\n","    return prediction"],"metadata":{"id":"nCyWjVxGWiDM","executionInfo":{"status":"ok","timestamp":1718242493636,"user_tz":240,"elapsed":303,"user":{"displayName":"Ken Constable","userId":"07550562182491091576"}}},"execution_count":150,"outputs":[]},{"cell_type":"markdown","source":["### Training\n","+ Parameter Efficient Fine Tuning (PEFT)\n","+ The LoRA methodology is used to fine-tune a small number of adaptors during training\n"," #### Why PEFT/LoRA?\n"," + This is the preferred method of many practitioners\n"," + It is effective at improving performance for task-specific fine-tuning\n"," + It uses much fewer resources than full instruction fine-tuning\n"," + It only trains a small fraction of model weights (~1%)\n"," + It prevents catastrophic forgetting when fine-tuning because the base model weights are unchanged.  The adaptors are merged with the original base model weights\n","  + There is only a small loss of performance when compared to full fine-tuning\n","\n"],"metadata":{"id":"OFgOfjvOGmIP"}},{"cell_type":"markdown","source":["#### Calculate Training Metrics\n","+ A convenience function to calculate model performance during training\n","+ Performance is evaluated after each epoch of training\n","+ This is a supervised binary classification task (we have the ground truth labels). Therefore, a classification accuracy measure can be used. The F1 score was selected to balance precision and recall\n","+ Steps\n","\t+ Download the F1 score from the evaluate library\n","\t+ Extract the logits from the prediction object\n","\t+ Infer the classification from the logits\n","\t+ Calculate the F1 score by comparing the predicted classification to the human-assigned classification"],"metadata":{"id":"-0xm6KOFmsMS"}},{"cell_type":"code","source":["def calc_training_metrics(pred):\n","  \"\"\" Calculate the evaluation metrics during training \"\"\"\n","\n","  # load the f1 metric from the evaluate library\n","  f1 = evaluate.load('f1')\n","\n","  # get the logits and labels from the prediction object\n","  logits, labels = pred\n","\n","  # classify by using the logit (assign using the largest value)\n","  predictions = np.argmax(logits, axis=-1)\n","\n","  # calculate the score\n","  score = f1.compute(predictions=predictions, references=labels)['f1']\n","\n","  return {'f1':score}\n"],"metadata":{"id":"4XPBp1oFuSjj","executionInfo":{"status":"ok","timestamp":1718241173062,"user_tz":240,"elapsed":701,"user":{"displayName":"Ken Constable","userId":"07550562182491091576"}}},"execution_count":124,"outputs":[]},{"cell_type":"markdown","source":["#### LoRA Configuration\n","+ **Key Parameters**\n","+ rank (r)\n"," + The dimensions of the adaptors to train\n"," + The values typically range from 3-32, with empirical observations that there is a diminishing return on Performance with a value > 10\n"," + The value is proportional to the number of parameters that can be tuned & the compute time requried\n","+ LoRA modules (target_modules)\n"," + Defines which layers the adaptors are added to in the base model\n"," + The options available depend on the topography of the base model\n"," + The documentation for each base model must be researched to determine which layers are available\n","+ LoRA dropout - regularization parameters\n","+ LoRA alpha - scaling factor for the weights.  Some [articles](https://www.anyscale.com/blog/fine-tuning-llms-lora-or-full-parameter-an-in-depth-analysis-with-llama-2) suggest this be set at 16 and not trained\n","+ task_type - the task type (summarization, classification, transaction etc.)\n","\n","+ Steps\n","\t+ Define the LoRA parameters in the LoraConfig object\n","\t+ Prepare the PEFT model from the base model + LoRA config object\n","\t+ View the number of trainable parameters in the PEFT model\n"],"metadata":{"id":"8WVpC8gDmj_Z"}},{"cell_type":"code","source":["# LoRA config\n","lora_config = LoraConfig(\n","    r = 8, # dimension of adaptors, rank\n","    target_modules = [\"q_lin\"], # add LoRA adaptors to these layers in the base model\n","    lora_alpha=16, # alpha scaling\n","    lora_dropout=0.05, # regularization, dropout probability\n","    task_type=TaskType.SEQ_CLS # text classification\n",")\n","\n","# Create the PEFT model from the base model and LoRA config\n","peft_model = get_peft_model(base_model, lora_config)\n","peft_model.print_trainable_parameters()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8FKwHj7LPx2T","executionInfo":{"status":"ok","timestamp":1718236960343,"user_tz":240,"elapsed":301,"user":{"displayName":"Ken Constable","userId":"07550562182491091576"}},"outputId":"6b85db01-f1f3-4ced-e127-b0d90bfbfa0a"},"execution_count":65,"outputs":[{"output_type":"stream","name":"stdout","text":["trainable params: 665,858 || all params: 67,620,868 || trainable%: 0.9847\n"]}]},{"cell_type":"markdown","source":["#### Training\n","This project aimed to demonstrate how to fine-tune LLMs for specific tasks using public datasets. As the focus was not on performance, no attempt at hyperparameter tuning was undertaken. In most instances, the default hyperparameter values were used\n","+ **Key Parameters**\n","+ output_dir - location to save trained adaptor weights\n","+ learning_rate -set to default\n","+ auto_find_batch_size - set to auto\n","+ Logging and evaluation were set to occur after each epoch\n","+ load_best_model_at_end - set to true to capture the best model from the epoch training\n","+ The data collator is used to automatically pad the text to the longest sequence in each batch"],"metadata":{"id":"I8kM3lmNwT6W"}},{"cell_type":"code","source":["# Data Collator: This function dynamically sets the padding during training\n","# ensures prompts of are equal length\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","# training config\n","config_training = TrainingArguments(\n","    output_dir=DIR_TRAIN,\n","    auto_find_batch_size=True,\n","    learning_rate=1e-3,\n","    logging_steps=1,\n","    num_train_epochs=10,\n","    eval_strategy='epoch',\n","    load_best_model_at_end=True\n",")\n","\n","# Trainer\n","trainer = Trainer(\n","    model=peft_model,\n","    args=config_training,\n","    data_collator = data_collator,\n","    train_dataset=tokenized_train,\n","    eval_dataset=tokenized_val,\n","    compute_metrics=calc_training_metrics\n",")\n","\n","# train\n","trainer.train()\n","\n","# save adaptor weights\n","trainer.save_model(DIR_MODEL)\n","# peft_model.push_to_hub('kconstable/sentiment-distlbert')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":498},"id":"Gk9o-Zp7QFzc","executionInfo":{"status":"ok","timestamp":1718237468688,"user_tz":240,"elapsed":438799,"user":{"displayName":"Ken Constable","userId":"07550562182491091576"}},"outputId":"71d4a071-f2f5-40af-bb5c-16a6ebfc66ed"},"execution_count":67,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1250/1250 07:16, Epoch 10/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.118700</td>\n","      <td>0.340488</td>\n","      <td>0.868687</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.330100</td>\n","      <td>0.256116</td>\n","      <td>0.924731</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.048100</td>\n","      <td>0.395238</td>\n","      <td>0.868687</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.016400</td>\n","      <td>0.400892</td>\n","      <td>0.875000</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.001300</td>\n","      <td>0.437728</td>\n","      <td>0.903226</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.010100</td>\n","      <td>0.427545</td>\n","      <td>0.903226</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.000000</td>\n","      <td>0.427249</td>\n","      <td>0.903226</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>0.005600</td>\n","      <td>0.505457</td>\n","      <td>0.893617</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>0.017100</td>\n","      <td>0.519009</td>\n","      <td>0.893617</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.012800</td>\n","      <td>0.510737</td>\n","      <td>0.893617</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]}]},{"cell_type":"markdown","source":["#### Merge Base Model & Adapters\n","+ The trained LoRA adaptors must be merged with the original base model\n","+ The resulting model consists of the base model plus the trained adaptors\n"],"metadata":{"id":"RF02lWhTGtxR"}},{"cell_type":"code","source":["# merge base model + peft adaptors\n","tuned_model = PeftModel.from_pretrained(\n","    base_model,\n","    DIR_MODEL, # LoRA adapters\n","    torch_dthype=torch.bfloat16,\n","    trust_remote_code=True,\n","    is_trainable=False\n","  )"],"metadata":{"id":"_R1GTHtaYQnr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Evaluate Model Performance\n","+ [hugging face evaluation metrics](https://huggingface.co/evaluate-metric)\n","+ This is a supervised binary classification task (we have the ground truth labels). Therefore, a classification accuracy measure can be used. The F1 score was selected to balance precision and recall\n","+ A function was defined to generate a classification for a list of samples from the test dataset\n","+ **Steps:**\n","\t+ Randomly select 500 examples from the test dataset (out of sample)\n","\t+ Compare the predictions to the human label for each example using the original base model and the fine-tuned model\n","\t+ Calculate the  overall F1 score for all 500 examples for each model\n","\t+ PEFT/LoRA fine-tuning increased the F1 score from 65% to 89%\n"],"metadata":{"id":"kobJ7rUW0dHb"}},{"cell_type":"code","source":["def evaluate_model(test_indexes, data, model, tokenizer):\n","  \"\"\" Generate classifications for each example in the test indexes \"\"\"\n","  # accumulator\n","  results = []\n","\n","  # loop through each test index in the dataset\n","  for idx in test_indexes:\n","    # get the human label and the generated classification\n","    example = data[idx]\n","    label = example['label']\n","    pred = get_response(example, model, tokenizer, verbose=False)\n","\n","    # accumuate results\n","    results.append({'idx':idx,'label':label,'pred':pred})\n","  return pd.DataFrame(results)"],"metadata":{"id":"PdiuS9CHKJir","executionInfo":{"status":"ok","timestamp":1718244751174,"user_tz":240,"elapsed":305,"user":{"displayName":"Ken Constable","userId":"07550562182491091576"}}},"execution_count":184,"outputs":[]},{"cell_type":"code","source":["# Select 500 examples from the test dataset\n","num_samples = test.num_rows-1\n","num_to_test = 500\n","test_indexes = random.sample(range(num_samples),num_to_test)\n","\n","\n","# Evaluate the Base Model\n","df_base = evaluate_model(test_indexes, test, original_model, tokenizer)\n","f1_base = f1.compute(predictions=df_base['pred'], references=df_base['label'])['f1']\n","print(f\"Base Model F1 Score: {f1_base*100:,.2f}%\")\n","\n","# Evaluate the Tuned Model\n","df_tuned = evaluate_model(test_indexes, test, tuned_model, tokenizer)\n","f1_tuned = f1.compute(predictions=df_tuned['pred'], references=df_tuned['label'])['f1']\n","print(f\"Tuned Model F1 Score: {f1_tuned*100:,.2f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mIHspOXFmcWb","executionInfo":{"status":"ok","timestamp":1718244767255,"user_tz":240,"elapsed":14238,"user":{"displayName":"Ken Constable","userId":"07550562182491091576"}},"outputId":"3c2b9f2e-f3ae-468e-d99c-b0d115e3d282"},"execution_count":185,"outputs":[{"output_type":"stream","name":"stdout","text":["Base Model F1 Score: 65.67%\n","Tuned Model F1 Score: 89.20%\n"]}]}]}