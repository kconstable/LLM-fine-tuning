{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install transformers datasets evaluate accelerate\n",
    "# %pip install torch torchdata\n",
    "# %pip intsall peft\n",
    "# %pip install rouge_score\n",
    "# %pip install loralib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from datetime import date\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, GenerationConfig, TrainingArguments, Trainer\n",
    "import torch\n",
    "import evaluate\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training directory\n",
    "DIR_TRAIN = f'./peft/train/{date.today().strftime(\"%Y-%m-%d\")}/'\n",
    "DIR_MODEL = \"./peft/models/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the Fine Tuning Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 12460\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_dataset = 'knkarthick/dialogsum'\n",
    "dataset = load_dataset(hf_dataset)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the Foundational Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'google/flan-t5-base'\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "tokenizer =AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = GenerationConfig(max_new_tokens = 50, do_sample = False)\n",
    "# config = GenerationConfig(max_new_tokens = 50, do_sample = True,temperature = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize the following conversation:\n",
      "\n",
      "Conversation: << this is a test prompt >> \n",
      "\n",
      " Summary:\n"
     ]
    }
   ],
   "source": [
    "def prepare_prompt(dialogue):\n",
    "    \"\"\"\"\"\"\n",
    "    prompt = \"Summarize the following conversation:\\n\\n\"\n",
    "    prompt += f\"Conversation: << {dialogue} >> \"\n",
    "    prompt += \"\\n\\n Summary:\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dialogue:40\n",
      "====================================================================================================\n",
      "#Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "\n",
      "Human Summary ----------------------------------------------------------------------------------------\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time. \n",
      "\n",
      "GenAI Summary ----------------------------------------------------------------------------------------\n",
      "The train is about to leave, but Tom is late.\n",
      "==================================================================================================== \n",
      "\n",
      "Dialogue:200\n",
      "====================================================================================================\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "Human Summary ----------------------------------------------------------------------------------------\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system. \n",
      "\n",
      "GenAI Summary ----------------------------------------------------------------------------------------\n",
      "#Person1#: You're considering upgrading your computer.\n",
      "==================================================================================================== \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_summaries(data, model, tokenizer, indexes):\n",
    "    \"\"\"\"\"\"\n",
    "    for idx in indexes:\n",
    "        dialogue = data['test'][idx]['dialogue']\n",
    "        human_summary = data['test'][idx]['summary']\n",
    "\n",
    "        # tokenize the input dialogue into tokens\n",
    "        tokens = tokenizer(prepare_prompt(dialogue), return_tensors='pt')\n",
    "\n",
    "        # get the model summary, decode from tokens back to text\n",
    "        genai_summary = tokenizer.decode(\n",
    "            model.generate(\n",
    "                inputs=tokens['input_ids'],\n",
    "                max_new_tokens = 50,\n",
    "            )[0],\n",
    "            skip_special_tokens = True\n",
    "        )\n",
    "\n",
    "        # print the summaries\n",
    "        print(f\"Dialogue:{idx}\")\n",
    "        print(\"=\"*100)\n",
    "        print(dialogue)\n",
    "        print(\"\\nHuman Summary\",'-'*88)\n",
    "        print(human_summary,'\\n')\n",
    "        print(\"GenAI Summary\",'-'*88)\n",
    "        print(genai_summary)\n",
    "        print(\"=\"*100,'\\n')\n",
    "\n",
    "generate_summaries(dataset,base_model,tokenizer,[40,200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PEFT/LoRA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(example):\n",
    "    \"\"\"\"\"\"\n",
    "    prompt_start = \"Summarize the following conversation: \\n\\n\"\n",
    "    prompt_end = \"\\n\\nSummary:\"\n",
    "    prompt = [prompt_start + d + prompt_end for d in example['dialogue']]\n",
    "    example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors='pt').input_ids\n",
    "    example['labels'] = tokenizer(example['summary'], padding=\"max_length\",truncation=True, return_tensors='pt').input_ids\n",
    "\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10f5e6a696b74e3e91136c2105edb533",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 12460\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 500\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 1500\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69b0d692f9ed43e18cd18de857ab2151",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/12460 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5cbc4b038374b0d89aa119a1091937b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 125\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 5\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 15\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# preprocess the data (prepare the prompts and tokenize the inputs)\n",
    "tokenized_datasets = dataset.map(preprocess, batched=True)\n",
    "tokenized_datasets =tokenized_datasets.remove_columns(['id','dialogue','summary','topic'])\n",
    "print(tokenized_datasets)\n",
    "\n",
    "# Filter to speed up training on CPU (keep every 100th observation)\n",
    "tokenized_datasets = tokenized_datasets.filter(lambda example, index:index % 100 ==0, with_indices=True)\n",
    "print(tokenized_datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r = 32,\n",
    "    lora_alpha=32,\n",
    "    target_modules = [\"q\",\"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(base_model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "config_training = TrainingArguments(\n",
    "    output_dir=DIR_TRAIN,\n",
    "    auto_find_batch_size=True,\n",
    "    learning_rate=0.003,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=1,\n",
    "    max_steps=1\n",
    ")\n",
    "trainer_peft = Trainer(\n",
    "    model=peft_model,\n",
    "    args=config_training,\n",
    "    train_dataset=tokenized_datasets['train']\n",
    ")\n",
    "\n",
    "# train\n",
    "trainer_peft.train()\n",
    "# trainer_peft.model.save_pretrained(DIR_MODEL)\n",
    "# tokenizer.save_pretrained(DIR_MODEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_peft.model.save_pretrained(DIR_MODEL)\n",
    "tokenizer.save_pretrained(DIR_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_base_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
